{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The goal of this challenge is to show you that we can also use 1D Convolutional Filters to scan sentences  (_instead of RNN_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Run this cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==4.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question**  Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    " **Warning**  The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:17:11.904356: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 16:17:11.933812: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Run this cell to load the data ###\n",
    "\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that to do NLP, you need to go through one of the following option.\n",
    "But in both cases, you can replace the recurrent layer (top part) by a CNN layer. \n",
    "\n",
    "We will try both options, starting from the one on the left were the embeddings are learned within the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Concatenate a Keras Embedding with a Conv1D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a fancy network here. \n",
    "\n",
    "Each of your words is represented by a vector of size N (the size of your embedding). Therefore, as a sentence is a sequence of words, it is represented by a matrix (number of words, N). Consequently, all your sentences are actually represented as matrices once embedded.\n",
    "\n",
    "If you think about it, an image is also a matrix. Said differently, you may represent your sentence of word as a matrix, where each column (or row, depending on how you want to look at it) is a word, and each row (or each column) corresponds to a coordinate in the embedding space. \n",
    "\n",
    "Well, in that case, as these are close to images, why not using convolutions on them? Yes, convolutions!\n",
    "But, be careful. In the case of images, convolutions are 2-dimensional as the filters scan these pictures left to right and top to bottom. In the case of our sentences, we want the kernel to move _only_ in word by word, hence left to right. It wouldn't make sense to move the kernels top to bottom as _one vector = one word_.\n",
    "\n",
    "So let's create a model that uses convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question**  You will need to prepare your data. First, tokenize them. Then, you need to pad them (use a value `maxlen` equal to 150). You also might need to compute the size of your vocabulary ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, maxlen=150, dtype='float32')\n",
    "X_test_pad = pad_sequences(X_test_token, maxlen=150, dtype='float32')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 1D Convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question**  Define a model that has :\n",
    "- an `Embedding` layer: \n",
    "    - `input_dim` = `vocab_size + 1`\n",
    "    - `output_dim` is the embedding space dimension\n",
    "    - `mask_zero` has to be set to `True`. \n",
    "    - Here, for computational reasons, set `input_length` to the maximum length of your observations (that you just defined in the previous question).\n",
    "- a `Conv1D` layer \n",
    "- a `Flatten` layer\n",
    "- a `Dense` layer\n",
    "- an output layer\n",
    "\n",
    "Compile the model accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "def init_cnn_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size + 1, output_dim=10, mask_zero=True, input_length=150))\n",
    "    model.add(layers.Conv1D(16, 3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(5,))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_cnn = init_cnn_model(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question**  Look at the number of parameters. You can compare it to the model that you had in previous exercises (esp. the first one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 150, 10)           304200    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 148, 16)           496       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2368)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 11845     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 316,547\n",
      "Trainable params: 316,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question**  Fit your model with a stopping criterion, and evaluate it on the test data.\n",
    "\n",
    "You will probably notice that it is ... **much faster** than RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 1s 4ms/step - loss: 0.6928 - accuracy: 0.5120 - val_loss: 0.6930 - val_accuracy: 0.5280\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5989 - accuracy: 0.8509 - val_loss: 0.6727 - val_accuracy: 0.5787\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.2839 - accuracy: 0.9634 - val_loss: 0.5973 - val_accuracy: 0.6880\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9954 - val_loss: 0.5575 - val_accuracy: 0.7213\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.5710 - val_accuracy: 0.7227\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9989 - val_loss: 0.5715 - val_accuracy: 0.7227\n",
      "Epoch 7/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5794 - val_accuracy: 0.7267\n",
      "Epoch 8/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5895 - val_accuracy: 0.7333\n",
      "Epoch 9/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5930 - val_accuracy: 0.7333\n",
      "The accuracy evaluated on the test set is of 73.200%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model_cnn.fit(X_train_pad, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "\n",
    "\n",
    "res = model_cnn.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Learn a Word2Vec representation, and then feed it into a NN with a `Conv1D` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the first part of the exercise, you were asked to jointly learn the embedding representation and the CNN convolution within the same network.\n",
    "\n",
    "Now, let's try to replace the RNN with a CNN for an architecture, as shown on the right side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " **Question**  Learn a Word2Vec model or load a trained one directly from GENSIM (transfer learning). Then, prepare your data as in the previous exercise. This question is quite long but it prepares you for real=world challenges. Don't worry, you have already done all the building bricks in the previous exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load a Word2Vec embedding\n",
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question**  Now, build a model that has a `Conv1D` layer, a `Flatten` layer, a `Dense` layer, and an `output` layer. Compile and fit it on the train data. You can then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.7760 - accuracy: 0.5451 - val_loss: 0.7068 - val_accuracy: 0.5907\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.7691 - val_loss: 0.6827 - val_accuracy: 0.6267\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8571 - val_loss: 0.7069 - val_accuracy: 0.6587\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.9166 - val_loss: 0.7811 - val_accuracy: 0.6320\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.2070 - accuracy: 0.9451 - val_loss: 0.8339 - val_accuracy: 0.6440\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.9743 - val_loss: 0.9477 - val_accuracy: 0.6360\n",
      "Epoch 7/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.1039 - accuracy: 0.9846 - val_loss: 1.0044 - val_accuracy: 0.6400\n",
      "The accuracy evaluated on the test set is of 61.640%\n"
     ]
    }
   ],
   "source": [
    "def init_cnn_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv1D(16, 3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(5,))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_cnn_2 = init_cnn_model_2()\n",
    "\n",
    "\n",
    "es_2 = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model_cnn_2.fit(X_train_pad_2, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es_2]\n",
    "         )\n",
    "\n",
    "\n",
    "res = model_cnn_2.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question**  You might be frustrated by the accuracy you got, this happens to us all at some point. Once you have tested your first iteration, you need to improve your models: by making them more complex, changing the parameters, stacking additional layers, and so on...\n",
    "\n",
    "Only practice and experimentation will get you there. So you can go back to your previous models, change them and try to get better results ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
