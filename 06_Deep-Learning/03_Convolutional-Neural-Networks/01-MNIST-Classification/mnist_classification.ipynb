{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_train_images = len(X_train)\n",
    "number_of_train_images_to_show = 3 # for example, but feel free to show more images\n",
    "random_list_of_images_to_show = np.random.randint(0, number_of_train_images , number_of_train_images_to_show)\n",
    "\n",
    "for i in random_list_of_images_to_show:\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 13:55:16.606953: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "X_train = expand_dims(X_train, axis=-1)\n",
    "X_test = expand_dims(X_test, axis=-1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                7850      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,264\n",
      "Trainable params: 9,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Conv2D\n",
    "first_layer_weights = 8 * (4*4) * 1 + 8\n",
    "first_layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second Conv2D\n",
    "second_layer_weights = 16 * (3*3) * 8 + 16\n",
    "second_layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7850"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third Conv2D\n",
    "third_layer_weights = 10 * 7 * 7 * 16 + 10\n",
    "third_layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dense Layer\n",
    "dense_layer_weights = 10 * 10 + 10\n",
    "dense_layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9264"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_number_of_weights = first_layer_weights + second_layer_weights + third_layer_weights + dense_layer_weights\n",
    "total_number_of_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1313/1313 [==============================] - 10s 8ms/step - loss: 0.3928 - accuracy: 0.8700 - val_loss: 0.1412 - val_accuracy: 0.9605\n",
      "Epoch 2/5\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.1169 - accuracy: 0.9655 - val_loss: 0.1046 - val_accuracy: 0.9689\n",
      "Epoch 3/5\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0825 - accuracy: 0.9755 - val_loss: 0.0919 - val_accuracy: 0.9727\n",
      "Epoch 4/5\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0674 - accuracy: 0.9801 - val_loss: 0.0735 - val_accuracy: 0.9779\n",
      "Epoch 5/5\n",
      "1313/1313 [==============================] - 9s 7ms/step - loss: 0.0561 - accuracy: 0.9831 - val_loss: 0.0767 - val_accuracy: 0.9763\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "es = EarlyStopping(patience = 5)\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train_cat,\n",
    "                    validation_split = 0.3,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 5,\n",
    "                    callbacks = [es],\n",
    "                    verbose = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0634 - accuracy: 0.9814\n",
      "The accuracy on the test set is of 98.14 %\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test, y_test_cat, verbose = 1 )\n",
    "print(f'The accuracy on the test set is of {res[1]*100:.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
